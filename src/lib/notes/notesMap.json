{
  "ddia-1": {
    "metadata": {
      "title": "Reliable, Scalable and Maintainable Applications",
      "description": "My notes on Chapter 1 of Designing Data-Intensive Applications, by Martin Kleppmann. I was familiar with a few of the concepts before reading, but it was a great refresher and I learned many new things. Basic Software Engineering knowledge is assumed.",
      "publishedAt": "2024-06-17T00:00:00Z"
    },
    "slug": "ddia-1",
    "content": "## Thinking about Data Systems\n\nOur applications use and operate on data for a variety of reasons, some of the most common ones being:\n\n- **Storage and Retrieval**: Databases, e.g. Postgres\n- **Caching**: Storing the result of an expensive operation to speed up reads, e.g. Redis\n- **Search Indexes**: Allowing users to query and filter large datasets, e.g., Google Search\n\nIt's common to lump databases, queues, caches etc. into the umbrella term, _data systems_. As new tools emerge, the boundaries have become blurred, for example, Apache Kafka is a message queue that guarantees database level durability.\n\nWhilst knowledge of data systems is crucial, it's more important to understand how to interface with them rather than implement them. Instead of building a storage engine from scratch, most programmers work on Application Code or Business logic, this is the part that \"glues\" several data systems and APIs together to serve customers or meet a business goal.\n\nTake an application like a todo list for example.\n\n<Image src=\"/ddia-1/todo-app-architecture.png\" alt=\"Todo App System Design\"/>\n\nNotice our application code, denoted by the server section, interfaces with Postgres and Redis, two different data systems. If users had a lot of long detailed todos, we could even connect to a 3rd data system optimized for full text search, companies like <Link target=\"_blank\" href=\"https://www.algolia.com/\">Algolia</Link> make this quite simple.\n\nWhen choosing what data systems to use and how to use them, Software Engineers care about functional requirements (the functionality that makes the app useful to users). Our todo app's functional requirements would be:\n\n- Reading, creating, editing, or deleting a todo\n- Signing in and out\n\nIf you're familiar with basic SQL, that's all accomplished, we have endpoints and Postgres tables that correlate to each requirement. However, there are implicit requirements that every software system shares. These are Reliability, Scalability and Maintainability.\n\n### Reliability\n\nThe ability of a system to function correctly and consistently over time, even in the presence of fault, this is also referred to as fault tolerance. Faults primarily manifest in 3 forms: Software errors, Hardware faults and Human errors often with overlap since humans, namely programmers, tend to cause software error.\n\n<Callout title=\"Faults vs Failures vs Errors\">\n  Errors may lead to faults, and faults may lead to failures. Though many people\n  use the terms interchangeably, especially fault and error, there are specific\n  differences. If there's a faulty part of your code, the application can work\n  fine if that part of the code doesn't run. However, if the code does run, you\n  get an error. Fails to handle that error, it becomes a fault, which may cause\n  the application to fail. Failure is typically non recoverable and the\n  application must restart.\n</Callout>\n\n#### Software Errors\n\nSoftware errors manifest most commonly in these forms:\n\n- **Bugs**: A software bug that causes application crashes when given specific input. For example, the leap second bug on June 30, 2012, caused many Linux systems to hang due to a kernel bug.\n- **Resource Leaks**: A runaway process that consumes all available memory, CPU, or disk space, causing the system to become unresponsive.\n- **Cascading Failures**: A small failure in one component triggering a series of failures in dependent components, such as a network switch failure causing database disconnections and subsequent application errors.\n\nIf you're familiar with Go, you'd know that it's crucial to handle a returned error.\n\n```go\nfunc main() {\n\tdb, err := sql.Open(\"postgres\", \"user=username dbname=mydb sslmode=disable\")\n\t/*\n\t if you don't check that the error is null and gracefully handle it, \n     further interactions with the database will crash the app\n\t */\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer db.Close()\n}\n```\n\nSimilarly, in Javascript, you should wrap code that can error in a try catch.\n\n```js\nasync function fetchData() {\n  try {\n    const response = await fetch(\"https://api.example.com/data\");\n    if (!response.ok) {\n      /* handle error */\n    }\n    const data = await response.json();\n    //...\n  } catch (error) {\n    console.error(\"Fetch error:\", error);\n    //handle error\n  }\n}\n```\n\n#### Hardware Faults\n\nAs an application developer, these are a bit more out of your control. Fortunately, one of the big selling points of Cloud Providers like AWS is that they abstract away the hardware for your. Cloud platforms are designed to prioritize flexibility and elasticity over single machine reliability, however, this does not mean they can’t occasionally occur.\n\nCommon Hardware Faults include:\n\n- **Disk Failures**: Hard disks have a mean time to failure (MTTF) of about 10 to 50 years, in large storage clusters, you might expect one disk to fail per day.\n- **Power Failures**: Power outages or fluctuations can cause system crashes and data corruption. Redundant power supplies and backup generators are common mitigations.\n- **Network Failures**: Unplugging the wrong network cable or router malfunctions can lead to network partitions, affecting the availability and consistency of distributed systems.\n\n#### Human Errors\n\nHuman errors are generally the most common cause of system failures. This is because humans interact with systems in complex ways and can make mistakes that automated systems might not predict or prevent. Studies have shown that configuration errors and operational mistakes by humans are leading causes of outages in large-scale systems.\n\nCommon Examples include:\n\n- **Configuration Errors**: Misconfiguring a system setting, such as a url connection string, can lead to service outages.\n- **Deployment Errors**: Accidentally deploying untested or buggy code to production can cause system failures.\n- **Operational Mistakes**: Actions like accidentally deleting critical data or failing to monitor system health can lead to major incidents.\n\nSome ways to mitigate human errors are, unit testing, integration testing, fully featured non production environments (_eg:_ _staging_) and monitoring and logging (_aka telemetry)._\n\n### Scalability\n\nScalability is concerned with how our our software handles _load_. The todo architecture from before would work for 100s to low 10,000s of concurrent users, however, as it grows to 100,000s we’d likely begin to put too much load on a single CPU. Imagine 1M people tried to add a todo at the same time. If one machine had to handle that, it’d likely crash and then all of those users would lose data.\n\nTo understand and plan for scalability, we describe the current load using load parameters. These could be requests per second, read/write ratios, active users, cache hit rates, etc. A great example of an application that faced scaling issue is Twitter. Twitter handles operations like posting tweets and displaying home timelines, which have different load characteristics.\n\nTLDR on how Twitter handles Scale\n\nIt’s like a mailbox. Twitter used a technique called fan-out. The intuition behind this is that people tend to tweet far less than they just open the app. Thus, it makes sense to send tweets to each follower at the time of posting instead of to querying the entire database whenever someone opens the app. Note that there are edge cases for celebrities with >1M followers where the logic must change since sending the tweet to so many people at once would make the experience bad for the tweeter.\n\n**Post Tweet Operation** \n- **Average Load**: 4.6k requests/second, peaking at 12k requests/second\n- **Description**: A user posts a tweet, which needs to be delivered to all their followers\n\n**View Timeline Operation**\n- **Average Load**: 300k requests/second\n- **Description**: Users view tweets posted by people they follow, requiring access to a personalized home timeline\n\n_Initially_, when a user requests their home timeline, the system looks up all the people they follow, retrieves all the tweets from those users, and merges them in real-time. This method was inefficient under heavy load, as merging tweets on-the-fly required significant computational resources, causing performance issues.\n\n<Image src=\"/ddia-1/twitter-1.png\" alt=\"Twitter's Initial Approach\"/>\n\n_To Optimize_, Tweets are now \"fanned out\" to each follower’s home timeline cache at the time of posting. This means each new tweet is immediately inserted into the home timeline cache of every follower. Reading the home timeline becomes faster since the tweets are pre-computed and stored in the follower’s cache.\n\n<Image src=\"/ddia-1/twitter-2.png\" alt=\"The 'fan-out' approach\"/>\n\n#### Describing Performance\n\nWhen describing the load on your system, you need to investigate what happens when the load increases. This can be approached in two ways:\n\n1. **Fixed Resources, Increasing Load**: How is system performance affected when load parameters increase while system resources (CPU, memory, network bandwidth) remain unchanged?\n2. **Increasing Resources, Fixed Performance**: How much do you need to increase resources to maintain performance when load parameters increase?\n\nThese questions require performance metrics to be properly answered. The performance of a system can be described using different metrics, depending on the system type:\n\n**Batch Processing Systems**\n\n- **Throughput**: Number of records processed per second or total time to run a job on a dataset of a certain size.\n\n**Online Systems**\n\n- **Response Time**: Time between a client sending a request and receiving a response.\n\n**Latency and Response Time**\n\n- **Latency**: The time a request is waiting to be handled.\n- **Response Time**: Includes actual processing time plus network and queuing delays.\n\nResponse time varies due to several factors, including network delays, server processing variations, and other random events. It’s better to consider response time as a distribution of values rather than a single number.\n\n**Response Time Distribution**\n\nResponse times should be evaluated using percentiles:\n\n- **Median (p50)**: The midpoint of the response times.\n- **95th Percentile (p95)**: 95% of requests are faster than this value.\n- **99th Percentile (p99)**: 99% of requests are faster than this value.\n\nHigh percentiles (tail latencies) are critical for user experience as they represent the slowest responses, which can significantly impact perceived performance. We don’t usually use the average response time since outliers can largely affect this.\n\nHigh percentiles are crucial in backend services because an end-user request often requires multiple backend calls. If any one of these backend calls is slow, the entire end-user request will be slow. This is known as **tail latency amplification**.\n\n**Approaches for Coping with Load**\n\n- **Vertical Scaling (Scaling Up)**: Moving to a more powerful machine or upgrading a part of the machine like the CPU.\n- **Horizontal Scaling (Scaling Out)**: Distributing the load across multiple machines, also known as a shared-nothing architecture.\n\nGood architectures usually combine both approaches. Systems that can automatically add computing resources in response to load increases are described as **elastic**.\n\n### Maintainability\n\nMaintainability is crucial since most software costs arise not from initial development but from ongoing maintenance tasks such as fixing bugs, keeping systems operational, adapting to new platforms, and adding features. Maintenance is often disliked due to the challenges of dealing with legacy systems and outdated platforms.\n\nTo minimize maintenance pain, three key design principles should be considered:\n\n1. **Operability**: This involves making it easy for operations teams to keep the system running smoothly. Good operability includes:\n   - Effective monitoring and quick restoration of services.\n   - Efficiently tracking and resolving system issues.\n   - Keeping software updated and ensuring security.\n   - Establishing good deployment and configuration practices.\n   - Maintaining system knowledge within the organization.\n2. **Simplicity**: Reducing complexity makes systems easier to understand and maintain. Complexity can lead to higher maintenance costs and increased risk of bugs. Achieving simplicity involves:\n   - Removing unnecessary complexity (accidental complexity).\n   - Using abstractions to hide implementation details and facilitate reuse.\n   - Ensuring that the system is easy to understand and reason about.\n3. **Evolvability**: Systems should be designed to adapt easily to changing requirements. This includes:\n   - Using Agile practices for small-scale changes and applying similar principles at a larger system level.\n   - Refactoring systems to accommodate new requirements or improved architectures.\n   - Keeping the system simple and modular to facilitate easy modifications.\n\nBy focusing on operability, simplicity, and evolvability, software systems can be designed to be more maintainable, reducing long-term costs and improving overall reliability and performance."
  }
}