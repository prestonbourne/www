{
  "aegis": {
    "slug": "aegis",
    "type": "work_route",
    "metadata": {
      "title": "Aegis",
      "description": "Built during my time at Cornell under the Microsoft Advisory program. Aegis is a concept startup that offers an API and dashboard for managing Large Language Models in a secure and compliant manner.",
      "publishedAt": "2024-04-23T20:47:40Z",
      "imageURL": "/work/aegis/aegis-presentation.webp"
    },
    "content": "<Video src=\"/work/aegis/dashboard-vid.mp4\" controls={false} autoPlay loop muted bleed/>\n\n## The Problem\n\nMicrosoft presented my team and I with a problem with the following prompt.\n> How might we make regulated industries more willing to adopt Large Language Models?\n\nMy project advisor, <PersonLink name=\"James Codella, PhD\" url=\"https://www.linkedin.com/in/jamescodella/\" />, pointed out that sectors like Finance and Health care are slow to adopt Large Language Models due to concerns around data privacy and security. A single LLM halucination could spell _disaster_ in these industries, even for internal use cases.\n\n## The Solution\n\nWe built Aegis. a two prongedservice that would sit between the person writing the prompt and the underlying model. The API would quickly categorize and return a risk assessment of each prompt. The consumer of the API would then be able to make a decision based on the risk assessment in the JSON response. \n\n### Detailed API Responses\n\nAn example response from the Aegis API looks like this, giving a risk score and a list of categories that the prompt may fall under with fine grained details for each category.\n\n```json\n{\n  \"status\": \"success\",\n  \"data\": { \n    \"prompt\": \"What is the capital of the moon?\",\n    \"analysis\": {\n      \"risk_score\": 0.5,\n      \"categories\": [\n        {\n            \"type\": \"hate_speech\",\n            \"confidence\": 0.1,\n        },\n        {\n            \"type\": \"sexual_content\",\n            \"confidence\": 0.1,\n        },\n        {\n            \"type\": \"self_harm\",\n            \"confidence\": 0.1,\n        },\n        {\n            \"type\": \"personal_data\",\n            \"confidence\": 0.1,\n        },\n        // ...\n      ]\n    }\n  },\n  \"metadata\": {\n    \"request_id\": \"rand_uuid_here\",\n    \"timestamp\": \"2024-01-01T00:00:00Z\",\n    \"service_version\": \"0.0.1\"\n  }\n}\n```\n\n### A Powerful Dashboard for LLM Management\n\n<Video src=\"/work/aegis/dashboard-vid.mp4\" autoPlay loop muted />\n\n## The Process\n\nA big part of the course and Cornell Tech's ethos is that these projects will primarily be evaluated on their merit as a product, rather than engineering achievements. This means that the team and I were in a unique position to be able to explore and research different approaches to the problem, iterate as needed, and make data driven decisions.\n\n### Research\n\nAs any good business should, we started by doing research. We looked at existing solutions, identified a few gaps in the market, and tried to measure the demand for products of this sort.\n\nWe started by doing desk research, looking at existing solutions and identifying a few gaps in the market, from this we were able to map out the standard industry practices for managing LLM safety.\n\n<Image src=\"/work/aegis/industry-map.png\" bleed borderless />\n\nFrom this we started to develop a better understanding of the problem space and we could move on to another phase of research, this time talking to experts in the field. Speaking with people who had experience working with regulated industries alongside empployees at AI safety companies, we were able to get a rough understanding of our user and customer personas.\n\n<Callout title=\"User vs Customer\">\n  The user is the peson who will be working with Aegis software on a day to day basis. The customer is the organization that will be purchasing Aegis to manage their LLM usage.\n</Callout>\n\nUser Profile:\n<Image src=\"/work/aegis/user-profile.png\" />\n\nCustomer Profile:\n<Image src=\"/work/aegis/customer-profile.png\" />\n\n## Design\n\n### System Architecture\n\nWith a better understanding of our users and customers, we were able to design a system that would meet their needs. \n\n<Image src=\"/work/aegis/info-architecture.png\" bleed borderless />\n\n### User Journey\n\nI was then able able to map out the journey through the Aegis dashboard. \n\n<Image src=\"/work/aegis/user-journey.png\" bleed borderless />\n\n### Design Iterations\n\nI was then able to map out the information architecture of the dashboard. Here a some early iterations that were vetted with users, customers and industry experts.\n\n<Image src=\"/work/aegis/des-1.png\" />\n<Image src=\"/work/aegis/des-2.png\" />\n\n## Conclusion & Learnings\n\nAegis represents a forward-thinking approach to LLM management, addressing the specific needs of regulated industries by offering a secure, compliant, and user-friendly solution. Through rigorous research, user-centered design, and iterative development, Aegis stands as a viable product concept ready to meet the challenges of LLM adoption in sensitive sectors.\n\nThis project was a great learning experience for me. I was able to wear many hats and implement my own designs. We collectively in the tech industry are still learning how to best design for LLM usage and for these new AI applications. It was exciting to be a part of that journey."
  },
  "cornell": {
    "slug": "cornell",
    "type": "work_route",
    "metadata": {
      "title": "My Exchange Student Experience",
      "description": "Cornell Tech was a life changing experience. I learned about product, business and academia in a way that was unmatched.",
      "publishedAt": "2024-05-23T20:47:40Z",
      "imageURL": "/work/cornell/logos.png"
    },
    "content": "I spent my senior year of college at Cornell University's NYC Campus, aka [Cornell Tech](https://tech.cornell.edu/).\n\nThere's a few learnings, highlights and things I want to brag about from the experience.\n\n## First off, the people.\n\n<Image\n  src=\"/work/cornell/people.jpg\"\n  alt=\"My group from the second semester. Raj, Tanisha, Sandhya, myself, Gabe and Dev.\"\n  caption=\"My group from the second semester. Raj, Tanisha, Sandhya, myself, Gabe and Dev.\"\n/>\n\nThe campus is primarily made up of graduate students, so I was a bit of a novelty. Despite that, my input or ideas were welcomed and celebrated.\n\nHere's everyone in the photo above, consider this an endorsement and personal attestment to their character and work ethics. I'd work with them again in a heartbeat.\n- <PersonLink name=\"Raj Sonani\" url=\"https://www.linkedin.com/in/raj-vijayakumar/\" />\n- <PersonLink name=\"Tanisha Kulkarni\" url=\"https://www.linkedin.com/in/tanisha-kulkarni/\" />\n- <PersonLink name=\"Sandhya Ramachandran\" url=\"https://www.linkedin.com/in/sandhya-ramachandran-a2743b65/\" />\n- <PersonLink name=\"Dev Makker\" url=\"https://www.linkedin.com/in/dev-makker/\" />\n- <PersonLink name=\"Gabriel Zhen\" url=\"https://www.linkedin.com/in/gabrielzhen/\" />\n\nThe professors were also awesome, they deeply cared about the material. Highlighting one isn't a jab at the others but something about Professor <PersonLink name=\"Guatam Ahuja\" url=\"https://business.cornell.edu/faculty-research/faculty/ga337/\" />'s class captivated me.\n\nProj. Ahuja was an entrepreneur before transitioning to academia. Despite my discipline being more technical, he gave me a deeper appreciation and understanding about the business and product strategy behind tech companies.\n\nHe can give an academic analysis on subjects like \"How to raise money from VCs\" or \"How to price your product\". Alongside why start ups succeed by rapidly iterating and learning from failure. Whilst larger incumbents would actually fail if they follow that paradigm.\n\nLast, but not least, my compnany advisors, <PersonLink name=\"Tylea Richard\" url=\"https://www.linkedin.com/in/tyleasimone/\" /> and <PersonLink name=\"James Codella\" url=\"https://www.linkedin.com/in/jamescodella/\" />, were phenomenal. I'll get more into what a company advisor is later, but they were always available to answer questions and provide guidance. True experts in their respective fields.\n\n## The Work\n\n### First Semester\n\nThe main attraction of Cornell Tech is that the work is heavily applied. Large tech, finance, consulting, real estate and pharma companies would send their employees to supervise and critique projects to act as the aforementioned company advisors.\n\nCompany's would provide a prompt, phrased as a \"How might we...?\" question, provide some resources and we'd work in teams of 4-6 students to come up with a solution.\n\nThe first semester was an algorithmic assignment to a company based on a survey of my interests and I ended up matching with Mozilla where <PersonLink name=\"Tylea Richard\" url=\"https://www.linkedin.com/in/tyleasimone/\" /> was my supervisor. Mozilla's challenge was \"How might we stop the spread of NCII in online communities?\"\n\n> NCII stands for Non Consensual Intimate Images. It's a term that encompasses several types of abuse, like revenge porn and now it's becoming an even more serious problem with the dawn of generative image models.\n\nI built FindMe, a web app that would allow users to understand and manage their digital footprint. It was a passion project of mine and I spent many weekends working on it. It was powered by a system that would aggregate the html of several websites where images tend to be posted and run the images against a pre-trained model to determine the probability of an image being you.\n\nWe built a waitlist for it with Supabase and got about ~50 sign ups in the first week.\n\n### Second Semester\n\nThe second semester I was paired with <PersonLink name=\"James Codella\" url=\"https://www.linkedin.com/in/jamescodella/\" /> from Microsoft where the challenge was \"How might we make Large Language Models more reliable in highly regulated industries?\"\n\nThis resulted in <Link href=\"/work/aegis\">Aegis</Link>. A 2 pronged service that would:\n\n1. Provide an API to run prompts against a model before forwarding them to the LLM.\n2. Provide a dashboard to understand the prompts flowing through the API and analyze for malicious intent.\n\nYou can read more about it [here](https://dl.acm.org/doi/10.1145/3581137.3615808).\n\nHonestly, I'm really proud of how the project turned out. One of the key metrrics for success was if the company advisors deemed the outcome something that their company would add to their product suite and [Microsoft did just that](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety).\n\n## The Campus\n\nNot much to say here, the campus was beautiful and getting to take the tram often was definitely a highlight.\n\n<Image\n  src=\"/work/cornell/tram.jpg\"\n/>\n\n## Quick Takeaways and Lessons\n\n- Engineers and Designers love building for the sake of the craft but a project is different than a product. Tech is a means to an end. If you don't understand that, your business will fail.\n- Nothing is \"not my job\". Be a CEO at heart, every flaw in the product is a reflection of you, even if it's not.\n- Creativity can be learned. Communication can be learned. Even the ability to learn can be learned.\n- Your idea is only as good as your ability to communicate it.\n- Your idea is only as good as your ability to see it materialize.\n- Good roadmaps evolve, they are fluid and change as you learn more about your users.\n- It is illegal to not maximize shareholder value, (I wish this was a joke) but you can get sued by shareholders for not cutting costs. A company is obligated to do everything in its power to turn a profit. If an entity does something _clearly_ unethical yet legal to cut costs, blame the law makers."
  },
  "ddia-1": {
    "slug": "ddia-1",
    "type": "work_route",
    "metadata": {
      "title": "Data Intensive Applications",
      "description": "My notes on Chapter 1 of Designing Data-Intensive Applications, by Martin Kleppmann. I was familiar with a few of the concepts before reading, but it was a great refresher and I learned many new things. Basic Software Engineering knowledge is assumed.",
      "publishedAt": "2024-06-17T00:00:00Z",
      "imageURL": "/work/ddia-1/todo-app-architecture.png"
    },
    "content": "## Thinking about Data Systems\n\nOur applications use and operate on data for a variety of reasons, some of the most common ones being:\n\n- **Storage and Retrieval**: Databases, e.g. Postgres\n- **Caching**: Storing the result of an expensive operation to speed up reads, e.g. Redis\n- **Search Indexes**: Allowing users to query and filter large datasets, e.g., Google Search\n\nIt's common to lump databases, queues, caches etc. into the umbrella term, _data systems_. As new tools emerge, the boundaries have become blurred, for example, Apache Kafka is a message queue that guarantees database level durability.\n\nWhilst knowledge of data systems is crucial, it's more important to understand how to interface with them rather than implement them. Instead of building a storage engine from scratch, most programmers work on Application Code or Business logic, this is the part that \"glues\" several data systems and APIs together to serve customers or meet a business goal.\n\nTake an application like a todo list for example.\n\n<Image src=\"/work/ddia-1/todo-app-architecture.png\"/>\n\nNotice our application code, denoted by the server section, interfaces with Postgres and Redis, two different data systems. If users had a lot of long detailed todos, we could even connect to a 3rd data system optimized for full text search, companies like <Link target=\"_blank\" href=\"https://www.algolia.com/\">Algolia</Link> make this quite simple.\n\nWhen choosing what data systems to use and how to use them, Software Engineers care about functional requirements (the functionality that makes the app useful to users). Our todo app's functional requirements would be:\n\n- Reading, creating, editing, or deleting a todo\n- Signing in and out\n\nIf you're familiar with basic SQL, that's all accomplished, we have endpoints and Postgres tables that correlate to each requirement. However, there are implicit requirements that every software system shares. These are Reliability, Scalability and Maintainability.\n\n### Reliability\n\nThe ability of a system to function correctly and consistently over time, even in the presence of fault, this is also referred to as fault tolerance. Faults primarily manifest in 3 forms: Software errors, Hardware faults and Human errors often with overlap since humans, namely programmers, tend to cause software error.\n\n<Callout title=\"Faults vs Failures vs Errors\">\n  Errors may lead to faults, and faults may lead to failures. Though many people\n  use the terms interchangeably, especially fault and error, there are specific\n  differences. If there's a faulty part of your code, the application can work\n  fine if that part of the code doesn't run. However, if the code does run, you\n  get an error. Fails to handle that error, it becomes a fault, which may cause\n  the application to fail. Failure is typically non recoverable and the\n  application must restart.\n</Callout>\n\n#### Software Errors\n\nSoftware errors manifest most commonly in these forms:\n\n- **Bugs**: A software bug that causes application crashes when given specific input. For example, the leap second bug on June 30, 2012, caused many Linux systems to hang due to a kernel bug.\n- **Resource Leaks**: A runaway process that consumes all available memory, CPU, or disk space, causing the system to become unresponsive.\n- **Cascading Failures**: A small failure in one component triggering a series of failures in dependent components, such as a network switch failure causing database disconnections and subsequent application errors.\n\nIf you're familiar with Go, you'd know that it's crucial to handle a returned error.\n\n```go\nfunc main() {\n\tdb, err := sql.Open(\"postgres\", \"user=username dbname=mydb sslmode=disable\")\n\t/*\n\t if you don't check that the error is null and gracefully handle it, \n     further interactions with the database will crash the app\n\t */\n\tif err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tdefer db.Close()\n}\n```\n\nSimilarly, in Javascript, you should wrap code that can error in a try catch.\n\n```js\nasync function fetchData() {\n  try {\n    const response = await fetch(\"https://api.example.com/data\");\n    if (!response.ok) {\n      /* handle error */\n    }\n    const data = await response.json();\n    //...\n  } catch (error) {\n    console.error(\"Fetch error:\", error);\n    //handle error\n  }\n}\n```\n\n#### Hardware Faults\n\nAs an application developer, these are a bit more out of your control. Fortunately, one of the big selling points of Cloud Providers like AWS is that they abstract away the hardware for your. Cloud platforms are designed to prioritize flexibility and elasticity over single machine reliability, however, this does not mean they can’t occasionally occur.\n\nCommon Hardware Faults include:\n\n- **Disk Failures**: Hard disks have a mean time to failure (MTTF) of about 10 to 50 years, in large storage clusters, you might expect one disk to fail per day.\n- **Power Failures**: Power outages or fluctuations can cause system crashes and data corruption. Redundant power supplies and backup generators are common mitigations.\n- **Network Failures**: Unplugging the wrong network cable or router malfunctions can lead to network partitions, affecting the availability and consistency of distributed systems.\n\n#### Human Errors\n\nHuman errors are generally the most common cause of system failures. This is because humans interact with systems in complex ways and can make mistakes that automated systems might not predict or prevent. Studies have shown that configuration errors and operational mistakes by humans are leading causes of outages in large-scale systems.\n\nCommon Examples include:\n\n- **Configuration Errors**: Misconfiguring a system setting, such as a url connection string, can lead to service outages.\n- **Deployment Errors**: Accidentally deploying untested or buggy code to production can cause system failures.\n- **Operational Mistakes**: Actions like accidentally deleting critical data or failing to monitor system health can lead to major incidents.\n\nSome ways to mitigate human errors are, unit testing, integration testing, fully featured non production environments (_eg:_ _staging_) and monitoring and logging (_aka telemetry)._\n\n### Scalability\n\nScalability is concerned with how our our software handles _load_. The todo architecture from before would work for 100s to low 10,000s of concurrent users, however, as it grows to 100,000s we’d likely begin to put too much load on a single CPU. Imagine 1M people tried to add a todo at the same time. If one machine had to handle that, it’d likely crash and then all of those users would lose data.\n\nTo understand and plan for scalability, we describe the current load using load parameters. These could be requests per second, read/write ratios, active users, cache hit rates, etc. A great example of an application that faced scaling issue is Twitter. Twitter handles operations like posting tweets and displaying home timelines, which have different load characteristics.\n\nTLDR on how Twitter handles Scale\n\nIt’s like a mailbox. Twitter used a technique called fan-out. The intuition behind this is that people tend to tweet far less than they just open the app. Thus, it makes sense to send tweets to each follower at the time of posting instead of to querying the entire database whenever someone opens the app. Note that there are edge cases for celebrities with >1M followers where the logic must change since sending the tweet to so many people at once would make the experience bad for the tweeter.\n\n**Post Tweet Operation** \n- **Average Load**: 4.6k requests/second, peaking at 12k requests/second\n- **Description**: A user posts a tweet, which needs to be delivered to all their followers\n\n**View Timeline Operation**\n- **Average Load**: 300k requests/second\n- **Description**: Users view tweets posted by people they follow, requiring access to a personalized home timeline\n\n_Initially_, when a user requests their home timeline, the system looks up all the people they follow, retrieves all the tweets from those users, and merges them in real-time. This method was inefficient under heavy load, as merging tweets on-the-fly required significant computational resources, causing performance issues.\n\n<Image src=\"/work/ddia-1/twitter-1.png\" caption=\"Twitter's Initial Approach\"/>\n\n_To Optimize_, Tweets are now \"fanned out\" to each follower’s home timeline cache at the time of posting. This means each new tweet is immediately inserted into the home timeline cache of every follower. Reading the home timeline becomes faster since the tweets are pre-computed and stored in the follower’s cache.\n\n<Image src=\"/work/ddia-1/twitter-2.png\" caption=\"The 'fan-out' approach\"/>\n\n#### Describing Performance\n\nWhen describing the load on your system, you need to investigate what happens when the load increases. This can be approached in two ways:\n\n1. **Fixed Resources, Increasing Load**: How is system performance affected when load parameters increase while system resources (CPU, memory, network bandwidth) remain unchanged?\n2. **Increasing Resources, Fixed Performance**: How much do you need to increase resources to maintain performance when load parameters increase?\n\nThese questions require performance metrics to be properly answered. The performance of a system can be described using different metrics, depending on the system type:\n\n**Batch Processing Systems**\n\n- **Throughput**: Number of records processed per second or total time to run a job on a dataset of a certain size.\n\n**Online Systems**\n\n- **Response Time**: Time between a client sending a request and receiving a response.\n\n**Latency and Response Time**\n\n- **Latency**: The time a request is waiting to be handled.\n- **Response Time**: Includes actual processing time plus network and queuing delays.\n\nResponse time varies due to several factors, including network delays, server processing variations, and other random events. It’s better to consider response time as a distribution of values rather than a single number.\n\n**Response Time Distribution**\n\nResponse times should be evaluated using percentiles:\n\n- **Median (p50)**: The midpoint of the response times.\n- **95th Percentile (p95)**: 95% of requests are faster than this value.\n- **99th Percentile (p99)**: 99% of requests are faster than this value.\n\nHigh percentiles (tail latencies) are critical for user experience as they represent the slowest responses, which can significantly impact perceived performance. We don’t usually use the average response time since outliers can largely affect this.\n\nHigh percentiles are crucial in backend services because an end-user request often requires multiple backend calls. If any one of these backend calls is slow, the entire end-user request will be slow. This is known as **tail latency amplification**.\n\n**Approaches for Coping with Load**\n\n- **Vertical Scaling (Scaling Up)**: Moving to a more powerful machine or upgrading a part of the machine like the CPU.\n- **Horizontal Scaling (Scaling Out)**: Distributing the load across multiple machines, also known as a shared-nothing architecture.\n\nGood architectures usually combine both approaches. Systems that can automatically add computing resources in response to load increases are described as **elastic**.\n\n### Maintainability\n\nMaintainability is crucial since most software costs arise not from initial development but from ongoing maintenance tasks such as fixing bugs, keeping systems operational, adapting to new platforms, and adding features. Maintenance is often disliked due to the challenges of dealing with legacy systems and outdated platforms.\n\nTo minimize maintenance pain, three key design principles should be considered:\n\n1. **Operability**: This involves making it easy for operations teams to keep the system running smoothly. Good operability includes:\n   - Effective monitoring and quick restoration of services.\n   - Efficiently tracking and resolving system issues.\n   - Keeping software updated and ensuring security.\n   - Establishing good deployment and configuration practices.\n   - Maintaining system knowledge within the organization.\n2. **Simplicity**: Reducing complexity makes systems easier to understand and maintain. Complexity can lead to higher maintenance costs and increased risk of bugs. Achieving simplicity involves:\n   - Removing unnecessary complexity (accidental complexity).\n   - Using abstractions to hide implementation details and facilitate reuse.\n   - Ensuring that the system is easy to understand and reason about.\n3. **Evolvability**: Systems should be designed to adapt easily to changing requirements. This includes:\n   - Using Agile practices for small-scale changes and applying similar principles at a larger system level.\n   - Refactoring systems to accommodate new requirements or improved architectures.\n   - Keeping the system simple and modular to facilitate easy modifications.\n\nBy focusing on operability, simplicity, and evolvability, software systems can be designed to be more maintainable, reducing long-term costs and improving overall reliability and performance."
  },
  "soundwave": {
    "slug": "soundwave",
    "type": "work_route",
    "metadata": {
      "title": "Soundwave",
      "description": "What if you could touch pixels? Most things on your screen exist in a digital plane, but Soundwave is a game that exists in the physical world.",
      "publishedAt": "2023-11-23T20:47:40Z",
      "imageURL": "/work/soundwave/cover.gif"
    },
    "content": "<Image\n    src=\"/work/soundwave/cover.gif\"\n    alt=\"The glove used in the project. Its a black glove with haptic feedback sensors at the fingertips with wires running up to a microcontroller at the wrist.\"\n/>\n\n- <Link href=\"https://github.com/prestonbourne/web-handtracking\"> Project Source </Link>\n- Collaborators: <PersonLink name=\"Zelong Li\" url=\"https://www.linkedin.com/in/lizeelong/\" />\n\nBuilt during my Junior year of college, Soundwave is a minigame that explores a novel method for computer interaction that moved away from traditional input devices.\n\nIt works by tracking the user's hand position in virtual space using a webcam. Additionally, a using a collision detection algorithm I was able to trigger haptics on designed glove allowing users to interact with digital content on a tactile level, effectively merging the digital and physical realms.\n\n\n<Image \n    src=\"/work/soundwave/glove.png\"\n    alt=\"The glove used in the project. Its a black glove with haptic feedback sensors at the fingertips with wires running up to a microcontroller at the wrist.\"\n/>\n\nSoundwave leverages websockets for real time communication between the microcontroller and the web application. \n\n<Image \n    borderless\n    bleed\n    src=\"/work/soundwave/architecture.png\"\n    alt=\"Architecture diagram of the system. The webcam captures video which is processed by an on device computer vision algorithm supplied by MediaPipe to track the hand position. The hand position is then sent to Web Socket server that broadcasts to the arduino microcontroller which controls the vibration of the glove.\"\n/>\n\n## Conclusion & Learnings\n\nThere's alot of potential for new forms of input and I'm excited to see where this line of research takes us in the coming years.\nSome ideas in this space that I think are interesting:\n- [Leap Motion](https://www.ultraleap.com/)\n- [Project Aria](https://www.projectaria.com/)\n- [Apple Gloves](https://www.patentlyapple.com/2022/03/apple-invents-mixed-reality-smart-gloves-designed-to-provide-vr-gamers-with-heightened-sensory-experiences.html)"
  },
  "undefined": {
    "type": "work_external",
    "url": "https://x.com/prestonbourne_/status/1828153247342764362",
    "metadata": {
      "title": "Hashicorp Navigation Bar",
      "description": "Aegis is a game about a group of heroes who must protect their city from a mysterious force.",
      "publishedAt": "2024-08-21T20:47:40Z",
      "imageURL": "/work/hashi-nav/cover.gif"
    }
  }
}